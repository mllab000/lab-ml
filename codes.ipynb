{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and dataset\n",
    "# Load libraries and the Iris dataset so every cell can use X, y, and Xs (scaled features)\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from scipy.stats import bernoulli, poisson\n",
    "import numpy.linalg as la\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.c_[X, y], columns=iris['feature_names']+['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross-validation:\n",
    "# K-Fold splits the data into k parts, trains on k-1 parts and tests on the remaining part,\n",
    "# repeating so each part is used as the test set once.\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "svc = SVC()\n",
    "scores = cross_val_score(svc, Xs, y, cv=kf)\n",
    "print('K-Fold SVC scores:', scores)\n",
    "print('Mean accuracy:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbe013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping:\n",
    "# Bootstrapping resamples the dataset with replacement to estimate variability (e.g., accuracy),\n",
    "# and uses out-of-bag samples to test each bootstrap model.\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "n = X.shape[0]\n",
    "boot_scores = []\n",
    "for i in range(200):\n",
    "    idx = np.random.choice(n, n, replace=True)\n",
    "    oob_idx = np.setdiff1d(np.arange(n), np.unique(idx))\n",
    "    if oob_idx.size == 0:\n",
    "        continue\n",
    "    dt.fit(X[idx], y[idx])\n",
    "    boot_scores.append(dt.score(X[oob_idx], y[oob_idx]))\n",
    "boot_scores = np.array(boot_scores)\n",
    "print('Bootstrap OOB mean accuracy:', boot_scores.mean())\n",
    "print('95% CI:', np.percentile(boot_scores, [2.5, 97.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and SVD:\n",
    "# PCA finds directions (principal components) that capture the most variance in the data.\n",
    "# SVD is a matrix factorization (similar to PCA) that decomposes the data into singular vectors and values.\n",
    "pca = PCA(n_components=2)\n",
    "Xp = pca.fit_transform(Xs)\n",
    "print('PCA shape:', Xp.shape)\n",
    "u, s, vh = la.svd(Xs, full_matrices=False)\n",
    "print('SVD shapes U,S,Vh:', u.shape, s.shape, vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (Linear Discriminant Analysis):\n",
    "# LDA finds linear combinations of features that best separate the classes.\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "Xl = lda.fit_transform(Xs, y)\n",
    "print('LDA shape:', Xl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli and Poisson distributions (random samples):\n",
    "# Bernoulli models binary outcomes (success/failure).\n",
    "# Poisson models counts (number of events in a fixed interval).\n",
    "print('Bernoulli samples (p=0.4):', bernoulli.rvs(0.4, size=10))\n",
    "print('Poisson samples (mu=3):', poisson.rvs(mu=3, size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561546c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes:\n",
    "# GaussianNB assumes features are normally distributed and independent given the class.\n",
    "gnb = GaussianNB()\n",
    "print('GaussianNB CV mean accuracy:', cross_val_score(gnb, Xs, y, cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM):\n",
    "# SVM finds a boundary (hyperplane) that separates classes with maximum margin.\n",
    "svm = SVC(kernel='rbf', gamma='scale')\n",
    "print('SVM CV mean accuracy:', cross_val_score(svm, Xs, y, cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors (KNN):\n",
    "# KNN classifies a sample by majority vote among its k nearest neighbors.\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "print('KNN CV mean accuracy:', cross_val_score(knn, Xs, y, cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest:\n",
    "# Random Forest is an ensemble of decision trees; it averages many trees to improve accuracy and reduce overfitting.\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "print('Random Forest CV mean accuracy:', cross_val_score(rf, Xs, y, cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90facfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree:\n",
    "# Decision Tree splits the data by asking feature-based yes/no questions to reach a decision.\n",
    "dt2 = DecisionTreeClassifier(random_state=0)\n",
    "print('Decision Tree CV mean accuracy:', cross_val_score(dt2, Xs, y, cv=kf).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple (univariate) Linear Regression:\n",
    "# Simple regression predicts a continuous target using a single feature (here petal length).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:, 2:3], y, test_size=0.2, random_state=1)\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X_train, y_train)\n",
    "y_pred = lr_simple.predict(X_test)\n",
    "print('Coef:', lr_simple.coef_, 'Intercept:', lr_simple.intercept_)\n",
    "print('MSE:', mean_squared_error(y_test, y_pred), 'R2:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19863dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression:\n",
    "# Multiple regression predicts a continuous target using multiple features.\n",
    "Xm_train, Xm_test, ym_train, ym_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "lr_multi = LinearRegression()\n",
    "lr_multi.fit(Xm_train, ym_train)\n",
    "ym_pred = lr_multi.predict(Xm_test)\n",
    "print('Coefs:', lr_multi.coef_, 'Intercept:', lr_multi.intercept_)\n",
    "print('MSE:', mean_squared_error(ym_test, ym_pred), 'R2:', r2_score(ym_test, ym_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression:\n",
    "# Logistic regression models the probability of class membership (classification) using a logistic function.\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "print('Logistic Regression CV mean accuracy:', cross_val_score(logreg, Xs, y, cv=kf).mean())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
